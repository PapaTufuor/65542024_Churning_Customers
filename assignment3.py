# -*- coding: utf-8 -*-
"""Assignment3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SueK8sHdmuNGWIuZ59IGrmk1I6NDCn9O
"""

!pip install scikeras
import pandas as pd
from google.colab import drive
import seaborn as sns
import numpy as np
import pickle
import joblib
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestRegressor
from keras.layers import Input, Dense, Dropout
from keras.models import Model
from scikeras.wrappers import KerasClassifier, KerasRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import train_test_split

from google.colab import drive
drive.mount('/content/drive')

df=pd.read_csv('/content/drive/My Drive/AIDataset/CustomerChurn_dataset.csv')
df

table=pd.DataFrame(df)

table.info()

items = [
    [
        col,
        table[col].dtype,
        table[col].nunique(),
        list(table[col].unique()[:3]),
        table[col].isnull().sum()
    ] for col in table
]

# Display the summary in a DataFrame
display(pd.DataFrame(data=items, columns=[
    'Attributes',
    'Data Type',
    'Total Unique',
    'Unique Sample',
    'Total Missing'
]))

table['TotalCharges']=pd.to_numeric(table.TotalCharges, errors='coerce')

table.describe()

table.nunique()

table.isnull().sum()

#identifying entries with a tenure of zero and dropping them since they will not contribute to churn prediction for existing customers
table[np.isnan(table['TotalCharges'])==True]

table=table.drop(labels=table[table['tenure']==0].index, axis=0)

table

table.describe()



fig,axs=plt.subplots(1, 3, figsize=(15, 5))

#tenure distribution

axs[0].hist(table['tenure'], bins=20, color='red', edgecolor='black')
axs[0].set_title('Tenure Distribution')
axs[0].set_xlabel('Tenure (months)')
axs[0].set_ylabel('Frequency')

#monthly charge distribution
axs[1].hist(table['MonthlyCharges'], bins=20, color='gold', edgecolor='black')
axs[1].set_title('Monthly Charges Distribution')
axs[1].set_xlabel('Monthly Charges')
axs[1].set_ylabel('Frequency')

#total charge distribution
axs[2].hist(table['TotalCharges'], bins=30, color='green', edgecolor='black')
axs[2].set_title('Total Charges Distribution')
axs[2].set_xlabel('Total Charges')
axs[2].set_ylabel('Frequency')

categorical= ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod', 'Churn']

for feature in categorical:
  table[feature].value_counts().plot(kind='bar')
  plt.title(feature)
  plt.show

plt.tight_layout()
plt.show()

#Creating a pie chart to show churn distribution
plt.figure(figsize=(6,6))
churn_counts=table['Churn'].value_counts()
plt.pie(churn_counts, labels=['Not Churned', 'Churned'], colors =[ 'purple', 'orange'], autopct= "%1.1f%%", shadow = True)
plt.title('Churn Distribution')
plt.show()

#Creating a correlation matrix

corr_matrix=table.corr()
sns.heatmap(corr_matrix)
plt.title('Correlation Matrix')
plt.show()

numerical=['MonthlyCharges', 'tenure', 'TotalCharges']
categorical= ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod', 'Churn']



for i in range(len(numerical)):
    for j in range(i+1, len(numerical)):
        plt.scatter(table[numerical[i]], table[numerical[j]])
        plt.xlabel(numerical[i])
        plt.ylabel(numerical[j])
        plt.title(f'{numerical[i]} vs {numerical[j]}')
        plt.show()

for cat_feature in categorical:
    for num_feature in numerical:
        sns.boxplot(x=cat_feature, y=num_feature, data=table)
        plt.title(f'{cat_feature} vs {num_feature}')
        plt.show()

sns.countplot(x='gender', hue= 'Churn', data=table)
plt.title('Churn Rate by Gender')

sns.countplot(x='SeniorCitizen', hue='Churn', data=table)
plt.title('Churn Rate for Senior Citizens vs Non-Senior Citizens')

fig, ax = plt.subplots(1, 2, figsize=(14, 4))

sns.countplot(x='Partner', hue='Churn', data=table, ax=ax[0])
ax[0].set_title('Churn Rate for Customers with and without Partners')

sns.countplot(x='Dependents', hue='Churn', data=table, ax=ax[1])
ax[1].set_title('Churn Rate for Customers with and without Dependents')

plt.tight_layout()
plt.show()

def getsize(df, col):
  a = df["Churn"][df["Churn"]=="Yes"].groupby(by=df[col]).count()[1]
  b = df["Churn"][df["Churn"]=="Yes"].groupby(by=df[col]).count()[0]
  c = df["Churn"][df["Churn"]=="No"].groupby(by=df[col]).count()[1]
  d = df["Churn"][df["Churn"]=="No"].groupby(by=df[col]).count()[0]
  l = [a, b, c, d]
  return l

plt.figure(figsize=(6, 6))
labels =["Churn: Yes","Churn:No"]
values = [1869,5163]
labels_gender = ["Y","N","Y","N"]
sizes_gender = getsize(table, 'Dependents')
colors = ['green', 'red']
colors_gender = ['#c2c2f0','#ffb3e6', '#c2c2f0','#ffb3e6']
explode = (0.3,0.3)
explode_gender = (0.1,0.1,0.1,0.1)
textprops = {"fontsize":15}
#Plot
plt.pie(values, labels=labels,autopct='%1.1f%%',pctdistance=1.08, labeldistance=0.8,colors=colors, startangle=90,frame=True, explode=explode,radius=10, textprops =textprops, counterclock = True, )
plt.pie(sizes_gender,labels=labels_gender,colors=colors_gender,startangle=90, explode=explode_gender,radius=7, textprops =textprops, counterclock = True, )
#Draw circle
centre_circle = plt.Circle((0,0),5,color='black', fc='white',linewidth=0)
fig = plt.gcf()
fig.gca().add_artist(centre_circle)

plt.title('Churn Distribution w.r.t Dependents: Having Dependents(Y), No Dependents(N)', fontsize=15, y=1.1)




plt.axis('equal')
plt.tight_layout()
plt.show()

plt.figure(figsize=(6, 6))
labels =["Churn: Yes","Churn:No"]
values = [1869,5163]
labels_gender = ["S","N","S","N"]
sizes_gender = [476,1393,666,4497]
colors = ['green', 'red']
colors_gender = ['#c2c2f0','#ffb3e6', '#c2c2f0','#ffb3e6']
explode = (0.3,0.3)
explode_gender = (0.1,0.1,0.1,0.1)
textprops = {"fontsize":15}
#Plot
plt.pie(values, labels=labels,autopct='%1.1f%%',pctdistance=1.08, labeldistance=0.8,colors=colors, startangle=90,frame=True, explode=explode,radius=10, textprops =textprops, counterclock = True, )
plt.pie(sizes_gender,labels=labels_gender,colors=colors_gender,startangle=90, explode=explode_gender,radius=7, textprops =textprops, counterclock = True, )
#Draw circle
centre_circle = plt.Circle((0,0),5,color='black', fc='white',linewidth=0)
fig = plt.gcf()
fig.gca().add_artist(centre_circle)

plt.title('Churn Distribution w.r.t Senior Citizen: Senior(S), Not Senior(N)', fontsize=15, y=1.1)

plt.axis('equal')
plt.tight_layout()
plt.show()

#From the data we can determine that
#Churn rate of senior citizens is lower than younger citizens
#Churn rate of non single customers is lower than single customers
# Churn of customers with dependents is lower than customers without dependents

# Create a subset of the data for non-senior male citizens with dependents and a partner
subset1 = table[(table['SeniorCitizen'] == 0) & (table['gender'] == 'Male') & (table['Dependents'] == 'Yes') & (table['Partner'] == 'Yes')]
plt.figure()
# Create a bar plot of the churn rate for this subset
sns.countplot(x='Churn', data=subset1)
plt.title('Churn Rate for Non-Senior Male Citizens with Dependents and a Partner')

# Create a subset of the data for senior male citizens without dependents and a partner
subset2 = table[(table['SeniorCitizen'] == 1) & (table['gender'] == 'Male') & (table['Dependents'] == 'No') & (table['Partner'] == 'No')]
plt.figure()
# Create a bar plot of the churn rate for this subset
sns.countplot(x='Churn', data=subset2)
plt.title('Churn Rate for Senior Male Citizens without Dependents and a Partner')



# Display the plots
plt.tight_layout()
plt.show()

# Customers with fiber optic internet service
subset4 = table[table['InternetService'] == 'Fiber optic']
sns.countplot(x='Churn', data=subset4)
plt.title('Churn Rate for Customers with Fiber Optic Internet Service')
plt.figure()


# Customers without internet service
subset5 = table[table['InternetService'] == 'No']
sns.countplot(x='Churn', data=subset5)
plt.title('Churn Rate for Customers without Internet Service')
plt.figure()

# Customers with DSL
subset5 = table[table['InternetService'] == 'DSL']
sns.countplot(x='Churn', data=subset5)
plt.title('Churn Rate for Customers with DSL')
plt.figure()

# Display the plots
plt.tight_layout()
plt.show()

# Commented out IPython magic to ensure Python compatibility.
#Demonstrating Tenure Distribution on Customer Churn

# %matplotlib inline


# Create a subplot with two columns and shared y-axis
fig, ax = plt.subplots(
    ncols=2,
    sharey=True,
    figsize=(7, 5),
    gridspec_kw={
        'width_ratios': [2, 5],
        'wspace': 0.05
    }
)

# Create a boxplot for 'tenure' vs. 'Churn'
sns.boxplot(
    data=table,
    y='tenure',
    x='Churn',
    ax=ax[0],
    palette='viridis'
).set_ylabel('Tenure (Months)')

# Create a histogram and KDE plot for 'tenure' with hue 'Churn'
sns.histplot(
    data=table,
    y='tenure',
    hue='Churn',
    ax=ax[1],
    bins=35,
    palette='viridis',
    kde=True
)

# Remove the spines (top and right lines) from the plot
sns.despine()


fig.suptitle('Tenure Distribution on Customer Churn')

#From the diagram, we can tell that customers with less tenure are more likely to churn.

"""

```
# This is formatted as code
```

# Feature Extraction
"""

table.nunique()

categorical_columns =['gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService',
            'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport','StreamingTV', 'StreamingMovies', 'Contract',
            'PaperlessBilling', 'PaymentMethod']

encoded_df=pd.get_dummies(table, columns=categorical_columns)
encoded_df

y=encoded_df['Churn']
y

print(encoded_df.columns)

encoded_df.drop('customerID',axis=1, inplace=True)
encoded_df.drop('Churn', axis=1, inplace=True)

encoded_df

scaler=StandardScaler()

X=pd.DataFrame(scaler.fit_transform(encoded_df), columns=encoded_df.columns)

X

encoder=LabelEncoder()

y=encoder.fit_transform(y)

y

X_train, X_temp, y_train, y_temp= train_test_split(X, y, test_size=0.2, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

model=RandomForestRegressor()

model.fit(X_train, y_train)

feature_importance=model.feature_importances_

df_train=pd.DataFrame({'Feature': encoded_df.columns, 'Importance': feature_importance})

df_train=df_train.sort_values(by='Importance', ascending=False)

df_train

top_features=df_train.head(5)["Feature"].tolist()

#However due to our findings from the Exploratory Data Analysis, we will include Senior Citizenship, dependence and relationship status to top features

top_features.extend(['SeniorCitizen', 'Dependents_Yes', 'Partner_Yes'])
top_features

X_train=X_train[top_features]
X_test=X_test[top_features]
X_val = X_val[top_features]

X_train

def create_model(dropout_rate=0.0):
    # Define the input layer
    inputs = Input(shape=(X_train.shape[1],))

    # Define the hidden layers
    x = Dense(64, activation='relu')(inputs)
    x = Dropout(dropout_rate)(x)  # Add dropout layer after the first hidden layer
    x = Dense(64, activation='relu')(x)
    x = Dropout(dropout_rate)(x)  # Add dropout layer after the second hidden layer

    # Define the output layer
    outputs = Dense(1, activation='sigmoid')(x)

    model = Model(inputs=inputs, outputs=outputs)

    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model
final_model = create_model(dropout_rate=0.5)

# Fit the model and save the history
history = final_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))

# Training & validation accuracy values
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')

# Training & validation loss values
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')

plt.tight_layout()
plt.show()

#Evaluating the model
loss, accuracy = final_model.evaluate(X_test, y_test)
print(f'Test Loss: {loss}')
print(f'Test Accuracy: {accuracy}')

def create_model(dropout_rate=0.0):
    # Define the input layer
    inputs = Input(shape=(X_train.shape[1],))

    # Define the hidden layers
    x = Dense(64, activation='relu')(inputs)
    x = Dropout(dropout_rate)(x)
    x = Dense(64, activation='relu')(x)
    x = Dropout(dropout_rate)(x)

    # #defining the output layer
    outputs = Dense(1, activation='sigmoid')(x)

    model = Model(inputs=inputs, outputs=outputs)

    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model



model = KerasClassifier(build_fn=create_model, dropout_rate=0.0, verbose=1)


param_grid = {'batch_size': [10, 20, 30, 40],
              'epochs': [10, 50, 70, 100],
              'dropout_rate' : [0.5, 0.6, 0.7, 0.8]}


grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)
grid_result = grid.fit(X_train, y_train)

print(f"Best: {grid_result.best_score_} using {grid_result.best_params_}")

# fitting the model and saving history
final_model = create_model(dropout_rate=0.5)
history = final_model.fit(X_train, y_train, epochs=70, batch_size=40, validation_data=(X_val, y_val))

# Plot training & validation accuracy values
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')

# Plot training & validation loss values
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')

plt.tight_layout()
plt.show()

# Evaluate the model
loss, accuracy = final_model.evaluate(X_test, y_test)
print(f'Test Loss: {loss}')
print(f'Test Accuracy: {accuracy}')

# Use the model to predict the probabilities of the positive class
y_pred_prob = final_model.predict(X_test)

# calculating the AUC score
auc = roc_auc_score(y_test, y_pred_prob)

print("The AUC Score is ", auc)

X_train

encoded_df

df_transform = encoded_df[top_features]
df_scaled = X = pd.DataFrame(scaler.fit_transform(df_transform), columns=df_transform.columns)
df_scaled

with open('top_features.pkl', 'wb') as file:
    pickle.dump(top_features, file)

joblib.dump(scaler,'scaler.pkl')

joblib.dump(final_model, 'model.pkl')